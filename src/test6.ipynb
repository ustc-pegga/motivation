{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "from measure import measure_model\n",
    "from torchstat import stat\n",
    "from torchsummary import summary\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "from model.op import * \n",
    "from measure import measure_model\n",
    "from ms_export import onnx_export\n",
    "from model.model import * \n",
    "from pruning import *\n",
    "from test6 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_model() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m intensity_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/hujie/code/motivation/checkpoint/mobilenet_3_imagenet25_pruning_in3-run15/ckpt.best.pth.tar\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m ratio \u001b[39m=\u001b[39m [\u001b[39m1.0\u001b[39m, \u001b[39m0.75\u001b[39m, \u001b[39m0.8125\u001b[39m, \u001b[39m0.78125\u001b[39m, \u001b[39m0.75\u001b[39m, \u001b[39m0.75\u001b[39m, \u001b[39m0.765625\u001b[39m, \u001b[39m0.7421875\u001b[39m, \u001b[39m0.7265625\u001b[39m, \u001b[39m0.7109375\u001b[39m, \u001b[39m0.703125\u001b[39m, \u001b[39m0.6796875\u001b[39m, \u001b[39m0.671875\u001b[39m, \u001b[39m0.4609375\u001b[39m, \u001b[39m0.203125\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m net \u001b[39m=\u001b[39m get_model(\u001b[39m\"\u001b[39;49m\u001b[39mmbv1\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mimagenet-25\u001b[39;49m\u001b[39m\"\u001b[39;49m,ratio,intensity_path)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(net)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_model() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "intensity_path = \"/home/hujie/code/motivation/checkpoint/mobilenet_3_imagenet25_pruning_in3-run15/ckpt.best.pth.tar\"\n",
    "ratio = [1.0, 0.75, 0.8125, 0.78125, 0.75, 0.75, 0.765625, 0.7421875, 0.7265625, 0.7109375, 0.703125, 0.6796875, 0.671875, 0.4609375, 0.203125]\n",
    "net = save_model(\"mbv1\",\"imagenet-25\",ratio,intensity_path)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hujie/code/motivation/data/test6/mbv1//ms/\n",
      "/home/hujie/code/motivation/data/test6/mbv1//onnx/\n",
      "CONVERT RESULT SUCCESS:0\n",
      "/home/hujie/code/motivation/data/test6/mbv1//ms/\n",
      "/home/hujie/code/motivation/data/test6/mbv1//onnx/\n",
      "CONVERT RESULT SUCCESS:0\n"
     ]
    }
   ],
   "source": [
    "cfg = [3,24,52,100,100,188,196,368,380,372,368,360,336,380,208]\n",
    "net = MobileNet(n_class=25)\n",
    "onnx_export(net,torch.randn(1,3,224,224),\"/home/hujie/code/motivation/data/test6/mbv1/\",\"imagenet-25_origin\")\n",
    "net = MobileNet(n_class=25, cfg=cfg)\n",
    "onnx_export(net,torch.randn(1,3,224,224),\"/home/hujie/code/motivation/data/test6/mbv1/\",\"imagenet-25_acc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
